---
name: AI Fix Test Suite Issues

# Centralized timeout configuration
env:
  WORKFLOW_TIMEOUT_MINUTES: 30
  CLI_INSTALL_TIMEOUT_MINUTES: 5
  AI_EXECUTION_TIMEOUT_MINUTES: ${{ vars.AI_EXECUTION_TIMEOUT_MINUTES || '10' }}

"on":
  workflow_dispatch:
    inputs:
      pull_request_number:
        description: 'Pull Request number to fix'
        required: true
        type: number

permissions:
  contents: write
  issues: write
  pull-requests: write
  actions: read

jobs:
  ai-fix-tests:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 30

    concurrency:
      group: ai-workflows-pr-${{ inputs.pull_request_number }}
      cancel-in-progress: false

    steps:
      - name: Get PR details
        id: pr_details
        env:
          GITHUB_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ inputs.pull_request_number }}
        run: |
          PR_INFO=$(gh pr view "$PR_NUMBER" --json headRefName,number)
          echo "head_ref=$(echo $PR_INFO | jq -r '.headRefName')" >> $GITHUB_OUTPUT
          echo "pr_number=$(echo $PR_INFO | jq -r '.number')" >> $GITHUB_OUTPUT

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_PAT }}
          ref: ${{ steps.pr_details.outputs.head_ref }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install OpenRouter dependencies
        run: |
          pip install openai==1.54.3 httpx==0.27.0


      - name: Install test dependencies
        run: |
          if [ -f "pyproject.toml" ]; then
            pip install -e ".[test]"
            pip install ruff==0.1.9 black==23.12.1 isort==5.13.2
          else
            pip install pytest PyYAML ruff==0.1.9 black==23.12.1 isort==5.13.2
          fi
          pip install pytest-xvfb pytest-cov coverage

      - name: Run tests and capture failures
        id: test_check
        run: |
          echo "## Test Issues Found" > test_issues.md
          echo "" >> test_issues.md

          # First run linting (tests depend on lint passing)
          echo "### Linting Check (Required for Tests):" >> test_issues.md
          if ! ruff check . > lint_output.txt 2>&1; then
            echo "❌ Linting must pass before tests can run:" >> test_issues.md
            echo '```' >> test_issues.md
            cat lint_output.txt >> test_issues.md
            echo '```' >> test_issues.md
          else
            echo "✅ Linting passed" >> test_issues.md
          fi

          if ! black --check . > black_output.txt 2>&1; then
            echo "❌ Black formatting issues:" >> test_issues.md
            echo '```' >> test_issues.md
            cat black_output.txt >> test_issues.md
            echo '```' >> test_issues.md
          fi

          if ! isort --check-only . > isort_output.txt 2>&1; then
            echo "❌ Import sorting issues:" >> test_issues.md
            echo '```' >> test_issues.md
            cat isort_output.txt >> test_issues.md
            echo '```' >> test_issues.md
          fi

          # Python tests
          echo "### Python Test Issues:" >> test_issues.md
          if [ -d "tests" ] && find tests -name "*.py" -type f | grep -q .; then
            if ! python -m pytest -v --tb=short > pytest_output.txt 2>&1; then
              echo '```' >> test_issues.md
              cat pytest_output.txt >> test_issues.md
              echo '```' >> test_issues.md
            else
              echo "✅ Python tests passed" >> test_issues.md
            fi
          else
            echo "No Python tests found" >> test_issues.md
          fi

          # Shell script tests
          echo "### Shell Script Test Issues:" >> test_issues.md
          if [ -f "./tests/test_change_detection.sh" ]; then
            if ! ./tests/test_change_detection.sh > shell_test_output.txt 2>&1; then
              echo '```' >> test_issues.md
              cat shell_test_output.txt >> test_issues.md
              echo '```' >> test_issues.md
            else
              echo "✅ Shell script tests passed" >> test_issues.md
            fi
          else
            echo "No shell script tests found" >> test_issues.md
          fi

          # YAML validation
          echo "### YAML Validation Issues:" >> test_issues.md
          if ! find . -name "*.yml" -o -name "*.yaml" | \
              xargs -I {} python -c "import yaml; yaml.safe_load(open('{}'))" \
              > yaml_test_output.txt 2>&1; then
            echo '```' >> test_issues.md
            cat yaml_test_output.txt >> test_issues.md
            echo '```' >> test_issues.md
          else
            echo "✅ YAML validation passed" >> test_issues.md
          fi

          # Markdown validation
          echo "### Markdown Validation Issues:" >> test_issues.md
          echo "Markdown validation requires markdownlint-cli2, will be checked in CI" >> test_issues.md

      - name: AI Fix Test Issues
        timeout-minutes: ${{ fromJSON(env.AI_EXECUTION_TIMEOUT_MINUTES) }}
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          AI_MODEL: ${{ vars.AI_MODEL || 'anthropic/claude-3.5-sonnet' }}
          PR_NUMBER: ${{ inputs.pull_request_number }}
        run: |
          # Create a comprehensive prompt for Claude
          cat << 'EOF' > fix_test_prompt.md
          I need help fixing test suite failures in this codebase. Here are the current test issues:

          $(cat test_issues.md)

          Please help me fix these test issues. The fixes should:
          1. Fix all failing tests while preserving their intent
          2. Address any linting issues that prevent tests from running
          3. Fix any import or syntax errors in test files
          4. Update test assertions that may be outdated
          5. Fix any missing test dependencies or setup issues
          6. Ensure test data and fixtures are properly configured
          7. Fix any async/sync test issues
          8. Address any environment-specific test failures
          9. Fix YAML and configuration file validation issues
          10. Maintain code functionality while fixing tests

          Original PR: Manual dispatch for PR #${PR_NUMBER}
          PR description: Manually triggered test fixes

          Please analyze the test failures and make the necessary file changes to fix these issues while preserving
          the test coverage and intent. Use the Edit tool to modify files directly.
          EOF

          # Use AI via OpenRouter to fix the issues
          echo "🤖 Using $AI_MODEL via OpenRouter to fix test issues..."
          python ./scripts/openrouter-ai-helper.py \
            --prompt-file fix_test_prompt.md \
            --output-file ai_test_response.md \
            --model "$AI_MODEL" \
            --title "AI Test Fix"

          echo "AI Response:"
          cat ai_test_response.md

      - name: Apply automatic fixes first
        run: |
          echo "## Applying automatic fixes..."

          # Auto-fix linting issues
          ruff check . --fix || echo "Ruff auto-fix completed"
          black . || echo "Black formatting applied"
          isort . || echo "Import sorting applied"

      - name: Verify fixes by running tests again
        run: |
          echo "## Verifying test fixes..."

          # Run linting first (required for tests)
          echo "Checking linting..."
          ruff check . || echo "Ruff still has issues"
          black --check . || echo "Black still has issues"
          isort --check-only . || echo "isort still has issues"

          # Run Python tests
          if [ -d "tests" ] && find tests -name "*.py" -type f | grep -q .; then
            echo "Running Python tests..."
            python -m pytest -v || echo "Python tests still failing"
          fi

          # Run shell script tests
          if [ -f "./tests/test_change_detection.sh" ]; then
            echo "Running shell script tests..."
            ./tests/test_change_detection.sh || echo "Shell script tests still failing"
          fi

          # Check YAML files
          echo "Validating YAML files..."
          find . -name "*.yml" -o -name "*.yaml" | \
            xargs -I {} python -c "import yaml; yaml.safe_load(open('{}'))" || \
            echo "YAML validation still failing"

      - name: Commit and push fixes
        env:
          HEAD_REF: ${{ steps.pr_details.outputs.head_ref }}
          PR_NUMBER: ${{ inputs.pull_request_number }}
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"

          git add -A
          if git diff --staged --quiet; then
            echo "No changes to commit"
            echo "CHANGES_MADE=false" >> $GITHUB_ENV
            exit 0
          fi

          echo "CHANGES_MADE=true" >> $GITHUB_ENV
          git commit -m "$(cat <<EOF
          🧪 AI Test Fix: Resolve test suite failures

          Automatically generated fixes for test issues reported in PR #${PR_NUMBER}

          Changes made:
          - Fixed failing Python tests
          - Resolved linting issues preventing test execution
          - Fixed shell script test failures
          - Corrected YAML validation issues
          - Updated test dependencies and imports
          - Fixed test assertions and data setup
          - Applied code formatting fixes

          🤖 Generated with OpenRouter AI

          Co-Authored-By: AI Assistant <noreply@openrouter.ai>
          EOF
          )"

          # Pull latest changes before pushing to avoid conflicts
          git pull --rebase origin "$HEAD_REF" || true
          git push origin "$HEAD_REF"

      - name: Remove triggering label
        env:
          GITHUB_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ inputs.pull_request_number }}
        run: |
          echo "Removing ai-fix-tests label from PR #$PR_NUMBER"
          gh pr edit "$PR_NUMBER" --remove-label "ai-fix-tests" --repo "${{ github.repository }}"
          echo "✅ Successfully removed ai-fix-tests label"

      - name: Comment on PR
        if: env.CHANGES_MADE == 'true'
        uses: actions/github-script@v7
        with:
          github-token: ${{ github.token }}
          script: |
            github.rest.issues.createComment({
              issue_number: ${{ inputs.pull_request_number }},
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `🧪 **AI Test Fixes Applied**

              I've automatically fixed the test suite failures directly in this PR!

              The AI has analyzed and fixed:
              - ✅ Failing Python tests
              - ✅ Linting issues preventing test execution
              - ✅ Shell script test failures
              - ✅ YAML validation issues
              - ✅ Test dependencies and imports
              - ✅ Test assertions and data setup
              - ✅ Code formatting issues

              The \`ai-fix-tests\` label has been removed. The fixes have been committed directly to this branch.

              Please review the changes and re-run any tests if needed! 🚀`
            });

      - name: Comment on PR if no changes needed
        if: env.CHANGES_MADE == 'false'
        uses: actions/github-script@v7
        with:
          github-token: ${{ github.token }}
          script: |
            github.rest.issues.createComment({
              issue_number: ${{ inputs.pull_request_number }},
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `🧪 **AI Test Check Complete**

              Good news! No test issues were found that need fixing.

              The \`ai-fix-tests\` label has been removed.

              All tests are passing! ✅`
            });
